{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "039e7059",
      "metadata": {
        "lines_to_next_cell": 0,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "039e7059",
        "outputId": "48da5969-7970-4b09-edfa-622448137f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.11/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.11/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import timm\n",
        "import spacy\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d369283c",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "d369283c"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "16d7f9e7",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "16d7f9e7"
      },
      "outputs": [],
      "source": [
        "seed = 59\n",
        "set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1b81de14",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "1b81de14"
      },
      "outputs": [],
      "source": [
        "train_data = []\n",
        "train_set_path = 'vqa_coco_dataset/vaq2.0.TrainImages.txt'\n",
        "\n",
        "with open(train_set_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        temp = line.split('\\t')\n",
        "        qa = temp[1].split('?')\n",
        "\n",
        "        if len(qa) == 3:\n",
        "            answer = qa[2].strip()\n",
        "        else:\n",
        "            answer = qa[1].strip()\n",
        "\n",
        "        data_sample = {\n",
        "            'image_path': temp[0][:-2],\n",
        "            'question': qa[0] + '?',\n",
        "            'answer': answer\n",
        "        }\n",
        "        train_data.append(data_sample)\n",
        "\n",
        "val_data = []\n",
        "val_set_path = 'vqa_coco_dataset/vaq2.0.DevImages.txt'\n",
        "\n",
        "with open(val_set_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        temp = line.split('\\t')\n",
        "        qa = temp[1].split('?')\n",
        "\n",
        "        if len(qa) == 3:\n",
        "            answer = qa[2].strip()\n",
        "        else:\n",
        "            answer = qa[1].strip()\n",
        "\n",
        "        data_sample = {\n",
        "            'image_path': temp[0][:-2],\n",
        "            'question': qa[0] + '?',\n",
        "            'answer': answer\n",
        "        }\n",
        "        val_data.append(data_sample)\n",
        "\n",
        "test_data = []\n",
        "test_set_path = 'vqa_coco_dataset/vaq2.0.TestImages.txt'\n",
        "\n",
        "with open(test_set_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        temp = line.split('\\t')\n",
        "        qa = temp[1].split('?')\n",
        "\n",
        "        if len(qa) == 3:\n",
        "            answer = qa[2].strip()\n",
        "        else:\n",
        "            answer = qa[1].strip()\n",
        "\n",
        "        data_sample = {\n",
        "            'image_path': temp[0][:-2],\n",
        "            'question': qa[0] + '?',\n",
        "            'answer': answer\n",
        "        }\n",
        "        test_data.append(data_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0bb78397",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "0bb78397"
      },
      "outputs": [],
      "source": [
        "eng = spacy.load('en_core_web_sm')\n",
        "\n",
        "def get_tokens(data_iter):\n",
        "    for sample in data_iter:\n",
        "        question = sample['question']\n",
        "        yield [token.text for token in eng.tokenizer(question)]\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    get_tokens(train_data),\n",
        "    min_freq=2,\n",
        "    specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
        "    special_first=True\n",
        ")\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d80e0c26",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "d80e0c26"
      },
      "outputs": [],
      "source": [
        "classes = set([sample['answer'] for sample in train_data])\n",
        "label2idx = {\n",
        "    cls_name: idx for idx, cls_name in enumerate(classes)\n",
        "}\n",
        "idx2label = {\n",
        "    idx: cls_name for idx, cls_name in enumerate(classes)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d8b6b621",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "d8b6b621"
      },
      "outputs": [],
      "source": [
        "def tokenize(question, max_seq_len):\n",
        "    tokens = [token.text for token in eng.tokenizer(question)]\n",
        "    sequence = [vocab[token] for token in tokens]\n",
        "    if len(sequence) < max_seq_len:\n",
        "        sequence += [vocab['<pad>']] * (max_seq_len - len(sequence))\n",
        "    else:\n",
        "        sequence = sequence[:max_seq_len]\n",
        "\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e2fc7d95",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "e2fc7d95"
      },
      "outputs": [],
      "source": [
        "class VQADataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data,\n",
        "        label2idx,\n",
        "        max_seq_len=20,\n",
        "        transform=None,\n",
        "        img_dir='vqa_coco_dataset/val2014-resised/'\n",
        "    ):\n",
        "        self.transform = transform\n",
        "        self.data = data\n",
        "        self.max_seq_len= max_seq_len\n",
        "        self.img_dir = img_dir\n",
        "        self.label2idx = label2idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.img_dir, self.data[index]['image_path'])\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        question = self.data[index]['question']\n",
        "        question = tokenize(question, self.max_seq_len)\n",
        "        question = torch.tensor(question, dtype=torch.long)\n",
        "\n",
        "        label = self.data[index]['answer']\n",
        "        label = label2idx[label]\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return img, question, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9ae73209",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "9ae73209"
      },
      "outputs": [],
      "source": [
        "data_transform = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(size=(224, 224)),\n",
        "        transforms.CenterCrop(size=180),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.GaussianBlur(3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(size=(224, 224,)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d83f7a20",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "d83f7a20"
      },
      "outputs": [],
      "source": [
        "train_dataset = VQADataset(\n",
        "    train_data,\n",
        "    label2idx=label2idx,\n",
        "    transform=data_transform['train']\n",
        ")\n",
        "val_dataset = VQADataset(\n",
        "    val_data,\n",
        "    label2idx=label2idx,\n",
        "    transform=data_transform['val']\n",
        ")\n",
        "test_dataset = VQADataset(\n",
        "    test_data,\n",
        "    label2idx=label2idx,\n",
        "    transform=data_transform['val']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1d47d6f7",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "1d47d6f7"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 256\n",
        "test_batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=test_batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=test_batch_size,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "aec0c59c",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "aec0c59c"
      },
      "outputs": [],
      "source": [
        "class VQAModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_classes,\n",
        "        img_model_name,\n",
        "        embedding_dim,\n",
        "        n_layers=2,\n",
        "        hidden_size=256,\n",
        "        drop_p=0.2\n",
        "    ):\n",
        "        super(VQAModel, self).__init__()\n",
        "        self.image_encoder = timm.create_model(\n",
        "            img_model_name,\n",
        "            pretrained=True,\n",
        "            num_classes=hidden_size\n",
        "        )\n",
        "\n",
        "        for param in self.image_encoder.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.lstm1 = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=drop_p\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_size * 3, hidden_size)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, img, text):\n",
        "        img_features = self.image_encoder(img)\n",
        "\n",
        "        text_emb = self.embedding(text)\n",
        "        lstm_out, _ = self.lstm1(text_emb)\n",
        "\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "\n",
        "        combined = torch.cat((img_features, lstm_out), dim=1)\n",
        "        x = self.fc1(combined)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1f3b3d98",
      "metadata": {
        "lines_to_next_cell": 0,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f3b3d98",
        "outputId": "83b037b1-d82f-4ded-eeb8-161bbd64983f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "n_classes = len(classes)\n",
        "img_model_name = 'resnet18'\n",
        "hidden_size = 256\n",
        "n_layers = 2\n",
        "embedding_dim = 128\n",
        "drop_p = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = VQAModel(\n",
        "    n_classes=n_classes,\n",
        "    img_model_name=img_model_name,\n",
        "    embedding_dim=embedding_dim,\n",
        "    n_layers=n_layers,\n",
        "    hidden_size=hidden_size,\n",
        "    drop_p=drop_p\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3aa5d1e4",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "3aa5d1e4"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for image, question, labels in dataloader:\n",
        "            image, question, labels = image.to(device), question.to(device), labels.to(device)\n",
        "            outputs = model(image, question)\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    loss = sum(losses) / len(losses)\n",
        "    acc = correct / total\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "def fit(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    epochs\n",
        "):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        batch_train_losses = []\n",
        "\n",
        "        model.train()\n",
        "        for idx, (images, questions, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            questions = questions.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, questions)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_train_losses.append(loss.item())\n",
        "\n",
        "        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_loss, val_acc = evaluate(\n",
        "            model,\n",
        "            val_loader,\n",
        "            criterion,\n",
        "            device\n",
        "        )\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'EPOCH {epoch + 1}:\\tTrain loss: {train_loss:.4f}\\tVal loss: {val_loss:.4f}\\tVal loss: {val_loss:.4f}\\tVal acc: {val_acc}')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b715933c",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "b715933c"
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\n",
        "epochs = 50\n",
        "\n",
        "scheduler_step_size = epochs * 0.8\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=lr\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=scheduler_step_size,\n",
        "    gamma=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a9328ab2",
      "metadata": {
        "lines_to_next_cell": 0,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9328ab2",
        "outputId": "1522e138-a026-46fb-89d6-835dc9b5d307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\tTrain loss: 0.6934\tVal loss: 0.6964\tVal loss: 0.6964\tVal acc: 0.48514344262295084\n",
            "EPOCH 2:\tTrain loss: 0.6808\tVal loss: 0.7007\tVal loss: 0.7007\tVal acc: 0.514344262295082\n",
            "EPOCH 3:\tTrain loss: 0.6463\tVal loss: 0.6847\tVal loss: 0.6847\tVal acc: 0.5548155737704918\n",
            "EPOCH 4:\tTrain loss: 0.5714\tVal loss: 0.6972\tVal loss: 0.6972\tVal acc: 0.5855532786885246\n",
            "EPOCH 5:\tTrain loss: 0.4859\tVal loss: 0.7139\tVal loss: 0.7139\tVal acc: 0.5732581967213115\n",
            "EPOCH 6:\tTrain loss: 0.4149\tVal loss: 0.7231\tVal loss: 0.7231\tVal acc: 0.610655737704918\n",
            "EPOCH 7:\tTrain loss: 0.3590\tVal loss: 0.7545\tVal loss: 0.7545\tVal acc: 0.6045081967213115\n",
            "EPOCH 8:\tTrain loss: 0.3033\tVal loss: 0.7789\tVal loss: 0.7789\tVal acc: 0.610655737704918\n",
            "EPOCH 9:\tTrain loss: 0.2773\tVal loss: 0.8036\tVal loss: 0.8036\tVal acc: 0.5917008196721312\n",
            "EPOCH 10:\tTrain loss: 0.2473\tVal loss: 0.7322\tVal loss: 0.7322\tVal acc: 0.6193647540983607\n",
            "EPOCH 11:\tTrain loss: 0.2285\tVal loss: 0.8595\tVal loss: 0.8595\tVal acc: 0.6224385245901639\n",
            "EPOCH 12:\tTrain loss: 0.2098\tVal loss: 0.8460\tVal loss: 0.8460\tVal acc: 0.610655737704918\n",
            "EPOCH 13:\tTrain loss: 0.1973\tVal loss: 0.8757\tVal loss: 0.8757\tVal acc: 0.6127049180327869\n",
            "EPOCH 14:\tTrain loss: 0.1729\tVal loss: 0.9399\tVal loss: 0.9399\tVal acc: 0.6347336065573771\n",
            "EPOCH 15:\tTrain loss: 0.1602\tVal loss: 0.9391\tVal loss: 0.9391\tVal acc: 0.632172131147541\n",
            "EPOCH 16:\tTrain loss: 0.1562\tVal loss: 1.2784\tVal loss: 1.2784\tVal acc: 0.6193647540983607\n",
            "EPOCH 17:\tTrain loss: 0.1605\tVal loss: 1.1836\tVal loss: 1.1836\tVal acc: 0.6219262295081968\n",
            "EPOCH 18:\tTrain loss: 0.1667\tVal loss: 1.1781\tVal loss: 1.1781\tVal acc: 0.6347336065573771\n",
            "EPOCH 19:\tTrain loss: 0.1619\tVal loss: 1.1828\tVal loss: 1.1828\tVal acc: 0.6075819672131147\n",
            "EPOCH 20:\tTrain loss: 0.1608\tVal loss: 1.1197\tVal loss: 1.1197\tVal acc: 0.5973360655737705\n",
            "EPOCH 21:\tTrain loss: 0.1550\tVal loss: 1.3252\tVal loss: 1.3252\tVal acc: 0.5896516393442623\n",
            "EPOCH 22:\tTrain loss: 0.1536\tVal loss: 1.5433\tVal loss: 1.5433\tVal acc: 0.6147540983606558\n",
            "EPOCH 23:\tTrain loss: 0.1518\tVal loss: 1.0473\tVal loss: 1.0473\tVal acc: 0.6065573770491803\n",
            "EPOCH 24:\tTrain loss: 0.1456\tVal loss: 1.2971\tVal loss: 1.2971\tVal acc: 0.6132172131147541\n",
            "EPOCH 25:\tTrain loss: 0.1532\tVal loss: 1.1178\tVal loss: 1.1178\tVal acc: 0.6234631147540983\n",
            "EPOCH 26:\tTrain loss: 0.1433\tVal loss: 1.1449\tVal loss: 1.1449\tVal acc: 0.6296106557377049\n",
            "EPOCH 27:\tTrain loss: 0.1402\tVal loss: 1.2327\tVal loss: 1.2327\tVal acc: 0.6193647540983607\n",
            "EPOCH 28:\tTrain loss: 0.1261\tVal loss: 1.1648\tVal loss: 1.1648\tVal acc: 0.608094262295082\n",
            "EPOCH 29:\tTrain loss: 0.1270\tVal loss: 1.3132\tVal loss: 1.3132\tVal acc: 0.6173155737704918\n",
            "EPOCH 30:\tTrain loss: 0.1195\tVal loss: 1.4620\tVal loss: 1.4620\tVal acc: 0.6198770491803278\n",
            "EPOCH 31:\tTrain loss: 0.1252\tVal loss: 1.3285\tVal loss: 1.3285\tVal acc: 0.6219262295081968\n",
            "EPOCH 32:\tTrain loss: 0.1261\tVal loss: 1.2791\tVal loss: 1.2791\tVal acc: 0.6265368852459017\n",
            "EPOCH 33:\tTrain loss: 0.1207\tVal loss: 1.5484\tVal loss: 1.5484\tVal acc: 0.6316598360655737\n",
            "EPOCH 34:\tTrain loss: 0.1189\tVal loss: 1.4117\tVal loss: 1.4117\tVal acc: 0.6224385245901639\n",
            "EPOCH 35:\tTrain loss: 0.1277\tVal loss: 1.6666\tVal loss: 1.6666\tVal acc: 0.5906762295081968\n",
            "EPOCH 36:\tTrain loss: 0.1425\tVal loss: 1.7058\tVal loss: 1.7058\tVal acc: 0.6121926229508197\n",
            "EPOCH 37:\tTrain loss: 0.1277\tVal loss: 1.4501\tVal loss: 1.4501\tVal acc: 0.6280737704918032\n",
            "EPOCH 38:\tTrain loss: 0.1250\tVal loss: 1.4269\tVal loss: 1.4269\tVal acc: 0.6244877049180327\n",
            "EPOCH 39:\tTrain loss: 0.1226\tVal loss: 1.3350\tVal loss: 1.3350\tVal acc: 0.6152663934426229\n",
            "EPOCH 40:\tTrain loss: 0.1200\tVal loss: 1.2433\tVal loss: 1.2433\tVal acc: 0.6331967213114754\n",
            "EPOCH 41:\tTrain loss: 0.1129\tVal loss: 1.3293\tVal loss: 1.3293\tVal acc: 0.632172131147541\n",
            "EPOCH 42:\tTrain loss: 0.1054\tVal loss: 1.3390\tVal loss: 1.3390\tVal acc: 0.632172131147541\n",
            "EPOCH 43:\tTrain loss: 0.1043\tVal loss: 1.3655\tVal loss: 1.3655\tVal acc: 0.6383196721311475\n",
            "EPOCH 44:\tTrain loss: 0.1001\tVal loss: 1.3977\tVal loss: 1.3977\tVal acc: 0.6378073770491803\n",
            "EPOCH 45:\tTrain loss: 0.1012\tVal loss: 1.4260\tVal loss: 1.4260\tVal acc: 0.639344262295082\n",
            "EPOCH 46:\tTrain loss: 0.0991\tVal loss: 1.4574\tVal loss: 1.4574\tVal acc: 0.6403688524590164\n",
            "EPOCH 47:\tTrain loss: 0.0972\tVal loss: 1.4890\tVal loss: 1.4890\tVal acc: 0.6413934426229508\n",
            "EPOCH 48:\tTrain loss: 0.0978\tVal loss: 1.5016\tVal loss: 1.5016\tVal acc: 0.6398565573770492\n",
            "EPOCH 49:\tTrain loss: 0.0977\tVal loss: 1.5265\tVal loss: 1.5265\tVal acc: 0.6398565573770492\n",
            "EPOCH 50:\tTrain loss: 0.0978\tVal loss: 1.5449\tVal loss: 1.5449\tVal acc: 0.6429303278688525\n"
          ]
        }
      ],
      "source": [
        "train_losses, val_losses = fit(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3268e909",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3268e909",
        "outputId": "06793c62-b6e5-4c45-ea8b-49b2c7c780b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on val/test dataset\n",
            "Val accuracy:  0.6429303278688525\n",
            "Test accuracy:  0.6444114737883284\n"
          ]
        }
      ],
      "source": [
        "val_loss, val_acc = evaluate(\n",
        "    model,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    device\n",
        ")\n",
        "test_loss, test_acc = evaluate(\n",
        "    model,\n",
        "    test_loader,\n",
        "    criterion,\n",
        "    device\n",
        ")\n",
        "\n",
        "print('Evaluation on val/test dataset')\n",
        "print('Val accuracy: ', val_acc)\n",
        "print('Test accuracy: ', test_acc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}